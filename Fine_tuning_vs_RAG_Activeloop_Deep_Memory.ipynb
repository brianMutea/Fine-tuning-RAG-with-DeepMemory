{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh2azAtvH3ET9MQ6+xUdd7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9effd54c6e54497991d3f9d29a4b9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_600625e68c574619b04594df3e05b3c7",
              "IPY_MODEL_4b247c3c9cb24c6eb76b7c2a27c9f707",
              "IPY_MODEL_2fbcda677b7a45eca36421b5a1f09d45"
            ],
            "layout": "IPY_MODEL_419ba7ca6d394d6380f022600753a40a"
          }
        },
        "600625e68c574619b04594df3e05b3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03e9a28d7a7b4e4ebfcebac67cec8322",
            "placeholder": "​",
            "style": "IPY_MODEL_84f505204e1c4990929060f8e540b01a",
            "value": "Generating embeddings: 100%"
          }
        },
        "4b247c3c9cb24c6eb76b7c2a27c9f707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f30d1523d6243e2b5860751c16208ee",
            "max": 37,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1883e4b5a80741f1a05f68a152e3dcf3",
            "value": 37
          }
        },
        "2fbcda677b7a45eca36421b5a1f09d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36643d5882a342d9b19e8af86b6a9ef5",
            "placeholder": "​",
            "style": "IPY_MODEL_6b9a32667e5649c484085d5a5ef53b09",
            "value": " 37/37 [00:01&lt;00:00, 32.89it/s]"
          }
        },
        "419ba7ca6d394d6380f022600753a40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03e9a28d7a7b4e4ebfcebac67cec8322": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f505204e1c4990929060f8e540b01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f30d1523d6243e2b5860751c16208ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1883e4b5a80741f1a05f68a152e3dcf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36643d5882a342d9b19e8af86b6a9ef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b9a32667e5649c484085d5a5ef53b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianMutea/Fine-tuning-RAG-with-DeepMemory/blob/main/Fine_tuning_vs_RAG_Activeloop_Deep_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning vs RAG: Activeloop Deep Memory"
      ],
      "metadata": {
        "id": "5xcFoGDyweqp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KRUrOH1swMjX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q llama-index==0.9.14.post3 openai==1.3.8 deeplake tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
        "\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = \"your_activeloop_token\""
      ],
      "metadata": {
        "id": "EoTAQq0lwbkh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download sample data"
      ],
      "metadata": {
        "id": "CMLD0B26xvbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p './data/paul_graham/'\n",
        "\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCBG-m6PxvCU",
        "outputId": "a95d1068-3224-4a61-b604-2b302e022c8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 75042  100 75042    0     0   281k      0 --:--:-- --:--:-- --:--:--  280k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import SimpleDirectoryReader\n",
        "from llama_index import VectorStoreIndex, ServiceContext, StorageContext\n",
        "from llama_index.vector_stores import DeepLakeVectorStore\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms import OpenAI\n",
        "import deeplake"
      ],
      "metadata": {
        "id": "fLsTaeYlzjga"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create LlamaIndex nodes/ chunks"
      ],
      "metadata": {
        "id": "UsSwEeqbyFU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size = 512, chunk_overlap=20)\n",
        "nodes = node_parser.get_nodes_from_documents(docs)\n",
        "\n",
        "\n",
        "# By default, the node/chunks ids are set to random uuids.\n",
        "# To ensure same id's per run, we manually set them.\n",
        "for idx, node in enumerate(nodes):\n",
        "  node.id_ = f\"node_{idx}\"\n",
        "\n",
        "print(f\"Number of Documents: {len(docs)}\")\n",
        "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n_79Z85yFBs",
        "outputId": "3fda2c49-741e-4c2b-bda2-3eb6784b93fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents: 1\n",
            "Number of nodes: 37 with the current chunk size of 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a local Deep Lake vector store"
      ],
      "metadata": {
        "id": "VrcU2tG0zYn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade deeplake"
      ],
      "metadata": {
        "id": "Vh4L0sK9ahG-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # create a DeepLakeVectorStore locally to store the vectors\n",
        "\n",
        "dataset_path = \"./data/paul_graham/deep_lake_db\" # for local DeepLakeVectorStore storage\n",
        "vector_store = DeepLakeVectorStore(dataset_path = dataset_path, ingestion_batch_size=1024, overwrite=True)\n",
        "\n",
        "# define the LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding() # embeddings\n",
        "\n",
        "\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes,\n",
        "                                service_context=service_context,\n",
        "                                storage_context=storage_context,\n",
        "                                show_progress=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235,
          "referenced_widgets": [
            "f9effd54c6e54497991d3f9d29a4b9ec",
            "600625e68c574619b04594df3e05b3c7",
            "4b247c3c9cb24c6eb76b7c2a27c9f707",
            "2fbcda677b7a45eca36421b5a1f09d45",
            "419ba7ca6d394d6380f022600753a40a",
            "03e9a28d7a7b4e4ebfcebac67cec8322",
            "84f505204e1c4990929060f8e540b01a",
            "5f30d1523d6243e2b5860751c16208ee",
            "1883e4b5a80741f1a05f68a152e3dcf3",
            "36643d5882a342d9b19e8af86b6a9ef5",
            "6b9a32667e5649c484085d5a5ef53b09"
          ]
        },
        "id": "6StngtUCxiLs",
        "outputId": "7cbb10c8-cbbf-4351-e4f1-d99a936461cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r\r\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/37 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9effd54c6e54497991d3f9d29a4b9ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37/37 [00:00<00:00, 200.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='./data/paul_graham/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            "   text       text      (37, 1)      str     None   \n",
            " metadata     json      (37, 1)      str     None   \n",
            " embedding  embedding  (37, 1536)  float32   None   \n",
            "    id        text      (37, 1)      str     None   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the local Vectore Store to Activeloop's platform and convert it into a managed database."
      ],
      "metadata": {
        "id": "prWUFIiR40Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local = dataset_path\n",
        "hub_path = f\"hub://academiaarticles/optimize_RAG_paul_graham\"\n",
        "hub_managed_path = f\"hub://academiaarticles/optimize_RAG_paul_graham_managed\"\n",
        "\n",
        "# upload the local vector store\n",
        "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
        "\n",
        "# create a managed vector store with different name\n",
        "ds = deeplake.deepcopy(hub_path,\n",
        "                       hub_managed_path,\n",
        "                       overwrite=True,\n",
        "                       runtime={\"tensor_db\": True})"
      ],
      "metadata": {
        "id": "0iZ_3uRqwrYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38371807-e21a-4efe-c236-8042b9e770ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying dataset: 96%|█████████▋| 27/28 [00:02<00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/academiaarticles/optimize_RAG_paul_graham\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying dataset: 96%|█████████▋| 27/28 [00:03<00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/academiaarticles/optimize_RAG_paul_graham_managed\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.visualize()"
      ],
      "metadata": {
        "id": "5nj4Hk7x6L81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "6d777cf5-ace2-4c34-8028-9ca328aeca17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HINT: Please forward the port - 35069 to your local machine, if you are running on the cloud.\n",
            " * Serving Flask app 'dataset_visualizer'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7ead8e58b5e0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"90%\"\n",
              "            height=\"800\"\n",
              "            src=\"https://app.activeloop.ai/visualizer/hub?url=hub://academiaarticles/optimize_RAG_paul_graham_managed&token=eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNTY0NzgwMCwiZXhwIjoxNzY3MTY0NTcyfQ.eyJpZCI6ImJyaWFubXV0ZWFrIn0.32d3ycgotGbqDrKI1LfQ818EAUzLJ1tjNoSnbiOG05KaWNsl5PCQ8WKIvZWFKr72V7iUUoPA1VOdUH8vzNUxoA\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate a Vector Store with the managed dataset that we just created"
      ],
      "metadata": {
        "id": "mjrVfz84rSxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True)"
      ],
      "metadata": {
        "id": "TJUAdBBKK8Dl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faddc5bc-4411-4270-d5c1-bb84e006840e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://academiaarticles/optimize_RAG_paul_graham_managed already exists, loading from the storage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a dataset of Queries and Documents\n",
        "\n",
        "Fetch `docs` and `ids` from the vector store"
      ],
      "metadata": {
        "id": "Q9yfcTUk_uhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch dataset docs and ids\n",
        "docs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)['value']\n",
        "ids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)['value']\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "e55vTauYLP3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063c9032-beaf-4f9f-faec-ce1d4d12ae74"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating a synthetic training dataset.\n",
        "We need labeled data (`query` and `document_id` pairs) to train a Deep Memory model. Sometimes, it can be difficult to get labeled data when you are starting from scratch. We will  generate queries/questions using gpt-3.5-turbo from our existing documents."
      ],
      "metadata": {
        "id": "z2F5NOK5iD7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def generate_question(text):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
        "                        You make sure the question can be answered by the text.\"},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text,\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except:\n",
        "        question_string = \"No question generated\"\n",
        "        return question_string"
      ],
      "metadata": {
        "id": "XXOqvzRf_rHx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_queries(docs: list[str], ids: list[str], n: int):\n",
        "\n",
        "    questions = []\n",
        "    relevances = []\n",
        "    pbar = tqdm(total=n)\n",
        "    while len(questions) < n:\n",
        "        # 1. randomly draw a piece of text and relevance id\n",
        "        r = random.randint(0, len(docs)-1)\n",
        "        text, label = docs[r], ids[r]\n",
        "\n",
        "        # 2. generate queries and assign and relevance id\n",
        "        generated_qs = [generate_question(text)]\n",
        "        if generated_qs == [\"No question generated\"]:\n",
        "            print(\"No question generated\")\n",
        "            continue\n",
        "\n",
        "        questions.extend(generated_qs)\n",
        "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
        "        pbar.update(len(generated_qs))\n",
        "\n",
        "    return questions[:n], relevances[:n]"
      ],
      "metadata": {
        "id": "8sAzweUh_rCW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Launch the query generation process with a desired size of 40 queries/questions"
      ],
      "metadata": {
        "id": "g6PbbjP2jGGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions, relevances = generate_queries(docs, ids, n=40)\n",
        "print(len(questions)) #40\n",
        "print(questions[0])"
      ],
      "metadata": {
        "id": "5diL--PC_q_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8050a444-1f23-4409-c962-7f193881d789"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:25<00:00,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "What is the name of the new company that the author started after Viaweb?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch Deep Memory Training"
      ],
      "metadata": {
        "id": "TUp7gr4Yjmzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain -U langchain-openai"
      ],
      "metadata": {
        "id": "6XLRov1NkD3E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "job_id = db.vectorstore.deep_memory.train(\n",
        "    queries=questions,\n",
        "    relevance = relevances,\n",
        "    embedding_function=embeddings.embed_documents,\n",
        ")"
      ],
      "metadata": {
        "id": "D6THkeKf_q9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c15083-0da6-478c-d7e8-4b787226cc52"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DeepMemory training job\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training data for deepmemory:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 40 embeddings in 1 batches of size 40:: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepMemory training job started. Job ID: 65c4ce63932a587875a12272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# During training you can check the status of the training run\n",
        "db.vectorstore.deep_memory.status(job_id=\"65c4c96b81f06997d7f98151\")"
      ],
      "metadata": {
        "id": "2hTGulYJ_q4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d4feef-c3df-48db-bb00-9a4fd9b88739"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/academiaarticles/optimize_RAG_paul_graham_managed\n",
            "--------------------------------------------------------------\n",
            "|                  65c4c96b81f06997d7f98151                  |\n",
            "--------------------------------------------------------------\n",
            "| status                     | pending                       |\n",
            "--------------------------------------------------------------\n",
            "| progress                   | None                          |\n",
            "--------------------------------------------------------------\n",
            "| results                    | not available yet             |\n",
            "--------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run a Deep Memory-enabled inference by setting `deep_memory=True`."
      ],
      "metadata": {
        "id": "Ngr9NpwWr9sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import OpenAI\n",
        "query = \"What are the main things Paul worked on before college?\"\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "vector_index = VectorStoreIndex.from_vector_store(db,\n",
        "                                                  service_context=service_context,\n",
        "                                                  storage_context=storage_context,\n",
        "                                                  show_progress=True)\n",
        "\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=3, db_kwargs={\"deep_memory\": True})\n",
        "\n",
        "response_vector = query_engine.query(query)\n",
        "print(response_vector.response)"
      ],
      "metadata": {
        "id": "rXXT5wd6_q1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71bbaff3-13c9-48d6-a9ba-a7ee64c28cee"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul worked on writing and programming before college.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate validation queries\n",
        "validation_questions, validation_relevances = generate_queries(docs, ids, n=40)\n",
        "\n",
        "# Launch the evaluation function\n",
        "recalls = db.vectorstore.deep_memory.evaluate(\n",
        "    queries=validation_questions,\n",
        "    relevance=validation_relevances,\n",
        "    embedding_function=embeddings.embed_documents,\n",
        "    qvs_params={\"log_queries\": True}\n",
        ")"
      ],
      "metadata": {
        "id": "jBXng5Yu_qzI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5e902bf-3f3d-4bf4-ce60-8d3ffc188ae1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [01:16<00:00,  1.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding queries took 0.48 seconds\n",
            "---- Evaluating without Deep Memory ---- \n",
            "Recall@1:\t  72.5%\n",
            "Recall@3:\t  92.5%\n",
            "Recall@5:\t  97.5%\n",
            "Recall@10:\t  100.0%\n",
            "Recall@50:\t  100.0%\n",
            "Recall@100:\t  100.0%\n",
            "---- Evaluating with Deep Memory ---- \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Deepmemory model not found in the dataset",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-b6ab9c762469>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Launch the evaluation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m recalls = db.vectorstore.deep_memory.evaluate(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mqueries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_questions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrelevance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_relevances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mDeepMemoryAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, relevance, queries, embedding_function, embedding, top_k, qvs_params)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0meval_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"with\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"without\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"---- Evaluating {eval_type} Deep Memory ---- \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             avg_recalls, queries_dict = recall_at_k(\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mindra_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mrelevance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py\u001b[0m in \u001b[0;36mrecall_at_k\u001b[0;34m(indra_dataset, relevance, query_embs, metric, top_k, use_model)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# Compute the cosine similarity between the query and all data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         view = get_view(\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0mquery_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeplake/core/vectorstore/deep_memory/deep_memory.py\u001b[0m in \u001b[0;36mget_view\u001b[0;34m(metric, query_emb, indra_dataset, return_tensors, tql_filter)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0mreturn_tensors_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0mtql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"SELECT * FROM (SELECT {return_tensors_str}, ROW_NUMBER() as indices, {metric}(embedding, ARRAY[{query_emb_str}]) as score {tql_filter_str} order by {metric}(embedding, ARRAY[{query_emb_str}]) desc limit 100)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m     \u001b[0mindra_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindra_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindra_view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Deepmemory model not found in the dataset"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A significant focus was on Activeloop's Deep Memory, which was integrated into RAG systems to enhance embedding retrieval accuracy. Deep Memory outperforms traditional methods like BM25 using lexical search and vector search using cosine similarity. We demonstrated it by getting higher recall values. It also efficiently reduces token usage in LLM prompts compared to query reformulation or transformation."
      ],
      "metadata": {
        "id": "RQ7i043bq77H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rr_M0iT5q7s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0MQV_UxJ_quG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}