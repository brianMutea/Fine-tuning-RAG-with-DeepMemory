{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMk0oef4ysgqQ6sTj1pdSFT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "50f9891c77094a31befef274eb9c4edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b73773b222140589ae10b5b780317c2",
              "IPY_MODEL_ee2e0e78ee2349e8a5bbf127b35c4a4a",
              "IPY_MODEL_83b9bbc523b640998b859700d91121de"
            ],
            "layout": "IPY_MODEL_649f79e7bc2e4d4c93db0cdd561a9f97"
          }
        },
        "8b73773b222140589ae10b5b780317c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93f314cfb37b4ca0aa74bf8db6d9ae4b",
            "placeholder": "​",
            "style": "IPY_MODEL_9b43f12c58bd4590af0fe3262ceec264",
            "value": "Generating embeddings: 100%"
          }
        },
        "ee2e0e78ee2349e8a5bbf127b35c4a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2d39d7c1cab47f2a80bf4b331387e4d",
            "max": 37,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_159f5973d3c54f4099184dea2134d4d9",
            "value": 37
          }
        },
        "83b9bbc523b640998b859700d91121de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3023df306a694d888f8298f360dc5ecd",
            "placeholder": "​",
            "style": "IPY_MODEL_a55a7e97119a4d4794ad11cb43acf861",
            "value": " 37/37 [00:01&lt;00:00, 33.04it/s]"
          }
        },
        "649f79e7bc2e4d4c93db0cdd561a9f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f314cfb37b4ca0aa74bf8db6d9ae4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b43f12c58bd4590af0fe3262ceec264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2d39d7c1cab47f2a80bf4b331387e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159f5973d3c54f4099184dea2134d4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3023df306a694d888f8298f360dc5ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a55a7e97119a4d4794ad11cb43acf861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianMutea/Fine-tuning-RAG-with-DeepMemory/blob/main/Fine_tuning_vs_RAG_Activeloop_Deep_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning vs RAG: Activeloop Deep Memory"
      ],
      "metadata": {
        "id": "5xcFoGDyweqp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KRUrOH1swMjX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q llama-index==0.9.14.post3 openai==1.3.8 deeplake tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = \"your_activeloop_token\""
      ],
      "metadata": {
        "id": "EoTAQq0lwbkh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download sample data"
      ],
      "metadata": {
        "id": "CMLD0B26xvbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p './data/paul_graham/'\n",
        "\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCBG-m6PxvCU",
        "outputId": "ba7017bc-3224-4cd0-a927-f0e479b6d230"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 75042  100 75042    0     0   331k      0 --:--:-- --:--:-- --:--:--  331k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import SimpleDirectoryReader\n",
        "from llama_index import VectorStoreIndex, ServiceContext, StorageContext\n",
        "from llama_index.vector_stores import DeepLakeVectorStore\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms import OpenAI\n",
        "import deeplake"
      ],
      "metadata": {
        "id": "fLsTaeYlzjga"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create LlamaIndex nodes/ chunks"
      ],
      "metadata": {
        "id": "UsSwEeqbyFU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size = 512, chunk_overlap=20)\n",
        "nodes = node_parser.get_nodes_from_documents(docs)\n",
        "\n",
        "\n",
        "# By default, the node/chunks ids are set to random uuids.\n",
        "# To ensure same id's per run, we manually set them.\n",
        "for idx, node in enumerate(nodes):\n",
        "  node.id_ = f\"node_{idx}\"\n",
        "\n",
        "print(f\"Number of Documents: {len(docs)}\")\n",
        "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n_79Z85yFBs",
        "outputId": "5ae4eb32-e18d-40d8-ae92-03e24eac7b8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents: 1\n",
            "Number of nodes: 37 with the current chunk size of 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a local Deep Lake vector store"
      ],
      "metadata": {
        "id": "VrcU2tG0zYn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade deeplake"
      ],
      "metadata": {
        "id": "Vh4L0sK9ahG-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # create a DeepLakeVectorStore locally to store the vectors\n",
        "\n",
        "dataset_path = \"./data/paul_graham/deep_lake_db\" # for local DeepLakeVectorStore storage\n",
        "vector_store = DeepLakeVectorStore(dataset_path = dataset_path, ingestion_batch_size=1024, overwrite=True)\n",
        "\n",
        "# define the LLM\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding() # embeddings\n",
        "\n",
        "\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes,\n",
        "                                service_context=service_context,\n",
        "                                storage_context=storage_context,\n",
        "                                show_progress=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235,
          "referenced_widgets": [
            "50f9891c77094a31befef274eb9c4edb",
            "8b73773b222140589ae10b5b780317c2",
            "ee2e0e78ee2349e8a5bbf127b35c4a4a",
            "83b9bbc523b640998b859700d91121de",
            "649f79e7bc2e4d4c93db0cdd561a9f97",
            "93f314cfb37b4ca0aa74bf8db6d9ae4b",
            "9b43f12c58bd4590af0fe3262ceec264",
            "f2d39d7c1cab47f2a80bf4b331387e4d",
            "159f5973d3c54f4099184dea2134d4d9",
            "3023df306a694d888f8298f360dc5ecd",
            "a55a7e97119a4d4794ad11cb43acf861"
          ]
        },
        "id": "6StngtUCxiLs",
        "outputId": "fa24c127-fab0-49f2-a5f4-ff20bb2115e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r\r\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/37 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50f9891c77094a31befef274eb9c4edb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 37/37 [00:00<00:00, 189.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='./data/paul_graham/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            "   text       text      (37, 1)      str     None   \n",
            " metadata     json      (37, 1)      str     None   \n",
            " embedding  embedding  (37, 1536)  float32   None   \n",
            "    id        text      (37, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the local Vectore Store to Activeloop's platform and convert it into a managed database."
      ],
      "metadata": {
        "id": "prWUFIiR40Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local = dataset_path\n",
        "hub_path = f\"hub://academiaarticles/optimize_RAG_paul_graham\"\n",
        "hub_managed_path = f\"hub://academiaarticles/optimize_RAG_paul_graham_managed\"\n",
        "\n",
        "# upload the local vector store\n",
        "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
        "\n",
        "# create a managed vector store with different name\n",
        "ds = deeplake.deepcopy(hub_path,\n",
        "                       hub_managed_path,\n",
        "                       overwrite=True,\n",
        "                       runtime={\"tensor_db\": True})"
      ],
      "metadata": {
        "id": "0iZ_3uRqwrYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da127e1-e03c-40bf-b48e-db9f3c5da0d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying dataset: 96%|█████████▋| 27/28 [00:31<00:01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/academiaarticles/optimize_RAG_paul_graham\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying dataset: 96%|█████████▋| 27/28 [00:41<00:01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/academiaarticles/optimize_RAG_paul_graham_managed\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.visualize()"
      ],
      "metadata": {
        "id": "5nj4Hk7x6L81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "7e7ea11e-e775-4ea9-a6a7-e27e4adb3535"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HINT: Please forward the port - 47751 to your local machine, if you are running on the cloud.\n",
            " * Serving Flask app 'dataset_visualizer'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7e1465ed18a0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"90%\"\n",
              "            height=\"800\"\n",
              "            src=\"https://app.activeloop.ai/visualizer/hub?url=hub://academiaarticles/optimize_RAG_paul_graham_managed&token=eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwNTY0NzgwMCwiZXhwIjoxNzY3MTY0NTcyfQ.eyJpZCI6ImJyaWFubXV0ZWFrIn0.32d3ycgotGbqDrKI1LfQ818EAUzLJ1tjNoSnbiOG05KaWNsl5PCQ8WKIvZWFKr72V7iUUoPA1VOdUH8vzNUxoA\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate a Vector Store with the managed dataset that we just created"
      ],
      "metadata": {
        "id": "mjrVfz84rSxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = DeepLakeVectorStore(dataset_path=hub_managed_path,\n",
        "                         overwrite=False,\n",
        "                         read_only=True,\n",
        "                         runtime={\"tensor_db\": True})"
      ],
      "metadata": {
        "id": "TJUAdBBKK8Dl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37284135-d960-460e-e44f-eaff92a42c15"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://academiaarticles/optimize_RAG_paul_graham_managed already exists, loading from the storage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a dataset of Queries and Documents\n",
        "\n",
        "Fetch `docs` and `ids` from the vector store"
      ],
      "metadata": {
        "id": "Q9yfcTUk_uhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch dataset docs and ids\n",
        "docs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)['value']\n",
        "ids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)['value']\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "e55vTauYLP3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9f423b-4723-4bf6-eaab-8811472632a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating a synthetic training dataset.\n",
        "We need labeled data (`query` and `document_id` pairs) to train a Deep Memory model. Sometimes, it can be difficult to get labeled data when you are starting from scratch. We will  generate queries/questions using gpt-3.5-turbo from our existing documents."
      ],
      "metadata": {
        "id": "z2F5NOK5iD7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def generate_question(text):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
        "                        You make sure the question can be answered by the text.\"},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text,\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except:\n",
        "        question_string = \"No question generated\"\n",
        "        return question_string"
      ],
      "metadata": {
        "id": "XXOqvzRf_rHx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_queries(docs: list[str], ids: list[str], n: int):\n",
        "\n",
        "    questions = []\n",
        "    relevances = []\n",
        "    pbar = tqdm(total=n)\n",
        "    while len(questions) < n:\n",
        "        # 1. randomly draw a piece of text and relevance id\n",
        "        r = random.randint(0, len(docs)-1)\n",
        "        text, label = docs[r], ids[r]\n",
        "\n",
        "        # 2. generate queries and assign and relevance id\n",
        "        generated_qs = [generate_question(text)]\n",
        "        if generated_qs == [\"No question generated\"]:\n",
        "            print(\"No question generated\")\n",
        "            continue\n",
        "\n",
        "        questions.extend(generated_qs)\n",
        "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
        "        pbar.update(len(generated_qs))\n",
        "\n",
        "    return questions[:n], relevances[:n]"
      ],
      "metadata": {
        "id": "8sAzweUh_rCW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Launch the query generation process with a desired size of 40 queries/questions"
      ],
      "metadata": {
        "id": "g6PbbjP2jGGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions, relevances = generate_queries(docs, ids, n=40)\n",
        "print(len(questions)) #40\n",
        "print(questions[0])"
      ],
      "metadata": {
        "id": "5diL--PC_q_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcccdb23-e0e9-4c07-942a-a09ea23a78d3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:48<00:00,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "What was the original idea for the startup, that the author and his colleague later realized was not a good one?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch Deep Memory Training"
      ],
      "metadata": {
        "id": "TUp7gr4Yjmzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain -U langchain-openai"
      ],
      "metadata": {
        "id": "6XLRov1NkD3E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "job_id = db.vectorstore.deep_memory.train(\n",
        "    queries=questions,\n",
        "    relevance = relevances,\n",
        "    embedding_function=embeddings.embed_documents,\n",
        ")"
      ],
      "metadata": {
        "id": "D6THkeKf_q9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056f7ce3-e645-4895-b251-1df775e757de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DeepMemory training job\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training data for deepmemory:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating 40 embeddings in 1 batches of size 40:: 100%|██████████| 1/1 [00:04<00:00,  4.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepMemory training job started. Job ID: 65c5c1a6b014de51622ae8e6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# During training you can check the status of the training run\n",
        "db.vectorstore.deep_memory.status(job_id=\"65c5c1a6b014de51622ae8e6\")"
      ],
      "metadata": {
        "id": "2hTGulYJ_q4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93f52d7-7622-4714-8f3b-0b8e20f0e440"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/academiaarticles/optimize_RAG_paul_graham_managed\n",
            "--------------------------------------------------------------\n",
            "|                  65c5c1a6b014de51622ae8e6                  |\n",
            "--------------------------------------------------------------\n",
            "| status                     | completed                     |\n",
            "--------------------------------------------------------------\n",
            "| progress                   | eta: 0.1 seconds              |\n",
            "|                            | recall@10: 87.50% (+12.50%)   |\n",
            "--------------------------------------------------------------\n",
            "| results                    | recall@10: 87.50% (+12.50%)   |\n",
            "--------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run a Deep Memory-enabled inference by setting `deep_memory=True`."
      ],
      "metadata": {
        "id": "Ngr9NpwWr9sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import OpenAI\n",
        "query = \"What are the main things Paul worked on before college?\"\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "vector_index = VectorStoreIndex.from_vector_store(db,\n",
        "                                                  service_context=service_context,\n",
        "                                                  storage_context=storage_context,\n",
        "                                                  show_progress=True)\n",
        "\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=3, db_kwargs={\"deep_memory\": True})\n",
        "\n",
        "response_vector = query_engine.query(query)\n",
        "print(response_vector.response)"
      ],
      "metadata": {
        "id": "rXXT5wd6_q1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf1e99e-0cf8-4c7f-e38a-7385686a5951"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul worked on writing and programming before college. He wrote short stories and also tried writing programs on the IBM 1401 using an early version of Fortran.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate validation queries\n",
        "validation_questions, validation_relevances = generate_queries(docs, ids, n=40)\n",
        "\n",
        "# Launch the evaluation function\n",
        "recalls = db.vectorstore.deep_memory.evaluate(\n",
        "    queries=validation_questions,\n",
        "    relevance=validation_relevances,\n",
        "    embedding_function=embeddings.embed_documents\n",
        ")"
      ],
      "metadata": {
        "id": "jBXng5Yu_qzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beeaba81-f0d0-4021-fd69-8f8ff3d1b5b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:43<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding queries took 1.83 seconds\n",
            "---- Evaluating without Deep Memory ---- \n",
            "Recall@1:\t  60.0%\n",
            "Recall@3:\t  92.5%\n",
            "Recall@5:\t  95.0%\n",
            "Recall@10:\t  100.0%\n",
            "Recall@50:\t  100.0%\n",
            "Recall@100:\t  100.0%\n",
            "---- Evaluating with Deep Memory ---- \n",
            "Recall@1:\t  42.5%\n",
            "Recall@3:\t  60.0%\n",
            "Recall@5:\t  75.0%\n",
            "Recall@10:\t  90.0%\n",
            "Recall@50:\t  100.0%\n",
            "Recall@100:\t  100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A significant focus was on Activeloop's Deep Memory, which was integrated into RAG systems to enhance embedding retrieval accuracy. Deep Memory outperforms traditional methods like BM25 using lexical search and vector search using cosine similarity. We demonstrated it by getting higher recall values. It also efficiently reduces token usage in LLM prompts compared to query reformulation or transformation."
      ],
      "metadata": {
        "id": "RQ7i043bq77H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rr_M0iT5q7s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0MQV_UxJ_quG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}